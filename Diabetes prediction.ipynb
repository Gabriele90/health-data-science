{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Diabetes_prediction",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KaXspOCZMtvr"
      },
      "source": [
        "**DIABETES PREDICTION**\n",
        "\n",
        "We'll try to build a machine learning model to accurately predict whether or not the patients in the dataset have diabetes or not."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXiyYeqqM4Am"
      },
      "source": [
        "**Importing required packages**\n",
        "\n",
        "The first part is to load all the packages needed in this comparison. Besides the basic packages like pandas, numpy, matplotlib we will import some of the scikit-learn packages for application of the MLAs and their comparison."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iAUXqDftMEeU"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.style.use(\"fivethirtyeight\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "azy_JWX5coJB",
        "outputId": "991d0b39-8e24-409c-e02a-76cf62bd6dd5"
      },
      "source": [
        "!pip install split_folders\r\n",
        "\r\n",
        "import splitfolders\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting split_folders\n",
            "  Downloading https://files.pythonhosted.org/packages/b8/5f/3c2b2f7ea5e047c8cdc3bb00ae582c5438fcdbbedcc23b3cc1c2c7aae642/split_folders-0.4.3-py3-none-any.whl\n",
            "Installing collected packages: split-folders\n",
            "Successfully installed split-folders-0.4.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwSNYrAxNMPL"
      },
      "source": [
        "**Importing the data set**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "id": "5IHEQpo4SHyu",
        "outputId": "3e451d7b-22f5-4d2f-cda7-08535d3c1c18"
      },
      "source": [
        "dataset=pd.read_csv('diabetes.csv')\n",
        "dataset.head(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-9904a21ff4b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'diabetes.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2010\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2011\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'diabetes.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtub_lSjxoHK"
      },
      "source": [
        "**EXPLORATORY DATA ANALYSIS**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-seDUIFx5Wi"
      },
      "source": [
        "***Descriptive statistics***\n",
        "\n",
        "Descriptive statistics analysis helps to describe the basic features of dataset and obtain a brief summary of the data.\n",
        "The describe() method in Pandas library helps us to have a brief summary of the dataset. It automatically calculates basic statistics for all numerical variables excluding NaN (we will come to this part later) values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 314
        },
        "id": "uv2vtq9cyAYm",
        "outputId": "2563b0e1-fe43-4b4f-ee25-8b18c064761c"
      },
      "source": [
        "dataset.describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Pregnancies</th>\n",
              "      <th>Glucose</th>\n",
              "      <th>BloodPressure</th>\n",
              "      <th>SkinThickness</th>\n",
              "      <th>Insulin</th>\n",
              "      <th>BMI</th>\n",
              "      <th>DiabetesPedigreeFunction</th>\n",
              "      <th>Age</th>\n",
              "      <th>Outcome</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>768.000000</td>\n",
              "      <td>768.000000</td>\n",
              "      <td>768.000000</td>\n",
              "      <td>768.000000</td>\n",
              "      <td>768.000000</td>\n",
              "      <td>768.000000</td>\n",
              "      <td>768.000000</td>\n",
              "      <td>768.000000</td>\n",
              "      <td>768.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>3.845052</td>\n",
              "      <td>120.894531</td>\n",
              "      <td>69.105469</td>\n",
              "      <td>20.536458</td>\n",
              "      <td>79.799479</td>\n",
              "      <td>31.992578</td>\n",
              "      <td>0.471876</td>\n",
              "      <td>33.240885</td>\n",
              "      <td>0.348958</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>3.369578</td>\n",
              "      <td>31.972618</td>\n",
              "      <td>19.355807</td>\n",
              "      <td>15.952218</td>\n",
              "      <td>115.244002</td>\n",
              "      <td>7.884160</td>\n",
              "      <td>0.331329</td>\n",
              "      <td>11.760232</td>\n",
              "      <td>0.476951</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.078000</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>99.000000</td>\n",
              "      <td>62.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>27.300000</td>\n",
              "      <td>0.243750</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>3.000000</td>\n",
              "      <td>117.000000</td>\n",
              "      <td>72.000000</td>\n",
              "      <td>23.000000</td>\n",
              "      <td>30.500000</td>\n",
              "      <td>32.000000</td>\n",
              "      <td>0.372500</td>\n",
              "      <td>29.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>6.000000</td>\n",
              "      <td>140.250000</td>\n",
              "      <td>80.000000</td>\n",
              "      <td>32.000000</td>\n",
              "      <td>127.250000</td>\n",
              "      <td>36.600000</td>\n",
              "      <td>0.626250</td>\n",
              "      <td>41.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>17.000000</td>\n",
              "      <td>199.000000</td>\n",
              "      <td>122.000000</td>\n",
              "      <td>99.000000</td>\n",
              "      <td>846.000000</td>\n",
              "      <td>67.100000</td>\n",
              "      <td>2.420000</td>\n",
              "      <td>81.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Pregnancies     Glucose  ...         Age     Outcome\n",
              "count   768.000000  768.000000  ...  768.000000  768.000000\n",
              "mean      3.845052  120.894531  ...   33.240885    0.348958\n",
              "std       3.369578   31.972618  ...   11.760232    0.476951\n",
              "min       0.000000    0.000000  ...   21.000000    0.000000\n",
              "25%       1.000000   99.000000  ...   24.000000    0.000000\n",
              "50%       3.000000  117.000000  ...   29.000000    0.000000\n",
              "75%       6.000000  140.250000  ...   41.000000    1.000000\n",
              "max      17.000000  199.000000  ...   81.000000    1.000000\n",
              "\n",
              "[8 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAlgmIOC32hJ"
      },
      "source": [
        "**Null values identyfication**\n",
        "\n",
        "When no data value is stored for a feature in a particular observation, we say this feature has missing values. Examining this is important because when some of your data is missing, it can lead to weak or biased analysis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "SgwT2PN8MNHR"
      },
      "source": [
        "dataset.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBY_fIFHNUUw"
      },
      "source": [
        "Checking the data set for any NULL values is very essential, as MLAs can not handle NULL values. We have to either eliminate the records with NULL values or replace them with the mean/median of the other values. we can see each of the variables are printed with number of null values. This data set has no null values so all are zero here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exDqiytY7ZUW"
      },
      "source": [
        "**Heatmap**\n",
        "\n",
        "Heatmap takes a rectangular data grid as input and then assigns a color intensity to each data cell based on the data value of the cell. This is a great way to get visual clues about the data.\n",
        "We will generate a heatmap of the output of isnull() in order to detect missing values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "sKoDNy4K7Qti"
      },
      "source": [
        "sns.heatmap(dataset.isnull())\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dR5wqUyPWTdt"
      },
      "source": [
        "Something is off. As you can see above, more than one columns has 0 as 'Insulin' value. Also, we have 0 as 'SkinThickness' value. It is not possible for value like BMI, Blood pressure or Skinthickness.\n",
        "\n",
        "Let's check how many zeros we have:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "8SoHcxFsWnnb"
      },
      "source": [
        "bp_zeros = dataset[dataset['BloodPressure'] == 0].shape[0]\n",
        "st_zeros = dataset[dataset['SkinThickness'] == 0].shape[0]\n",
        "glucos_zeros = dataset[dataset['Glucose'] == 0].shape[0]\n",
        "bmi_zeros = dataset[dataset['BMI'] == 0].shape[0]\n",
        "insulin_zeros = dataset[dataset['Insulin'] == 0].shape[0]\n",
        "print(f\"Number of zeros 'BloodPressure' column have : {bp_zeros}\")\n",
        "print(f\"Number of zeros 'SkinThickness' column have : {st_zeros}\")\n",
        "print(f\"Number of zeros 'Glucose' column have : {glucos_zeros}\")\n",
        "print(f\"Number of zeros 'BMI' column have : {bmi_zeros}\")\n",
        "print(f\"Number of zeros 'Insulin' column have : {insulin_zeros}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZirLuk4PW9oI"
      },
      "source": [
        "Slightly more than half of the insulin column has zero as value."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPc9Wij7q3AX"
      },
      "source": [
        "**Histograms**\n",
        "\n",
        "Histogram shows us the frequency distribution of a variable. It partitions the spread of numeric data into parts called as “bins” and then counts the number of data points that fall into each bin"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "PnEY0nF8W-oL"
      },
      "source": [
        "dataset.hist(color='red', figsize=(20,15));"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z97ieD1lXKjZ"
      },
      "source": [
        "SkinThickness, Insulin is right skewed.\n",
        "\n",
        "BMI, and BloodPressure is normally distributed.\n",
        "\n",
        "Glucose is left skewed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "koge472lXVD4"
      },
      "source": [
        "**FEATURE ENGINEERING**\n",
        "\n",
        "*Standardization*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGzl210oXbqt"
      },
      "source": [
        "As it is normally distributed like BMI, and BloodPressure. If we fill zeros with median of that columns, we wouldn't disrupt the data.\n",
        "\n",
        "For left, and right skewed data, we can fill zeros with median of that columns."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Asr8Uu4KXm_N"
      },
      "source": [
        "data_2 = dataset.copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "HTN4T7GxXm4O"
      },
      "source": [
        "data_2['Insulin'].replace(0, data_2['Insulin'].median(), inplace=True)\n",
        "data_2['SkinThickness'].replace(0, data_2['SkinThickness'].median(), inplace=True)\n",
        "data_2['BMI'].replace(0, data_2['BMI'].mean(), inplace=True)\n",
        "data_2['Glucose'].replace(0, data_2['Glucose'].median(), inplace=True)\n",
        "data_2['BloodPressure'].replace(0, data_2['BloodPressure'].mean(), inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "WbxfWn4dXtOw"
      },
      "source": [
        "data_2.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "yX_7U35jYtR1"
      },
      "source": [
        "data_2.describe().T"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8JS5U4zXK4r"
      },
      "source": [
        "**Correlation**\n",
        "\n",
        "Now, we can check the correlations using an heatmap."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "g-klDKrxY2vD"
      },
      "source": [
        "def half_corr_heatmap(data, title=None):\n",
        "    plt.figure(figsize=(9,9))\n",
        "    sns.set(font_scale=1)\n",
        "    \n",
        "    mask = np.zeros_like(data.corr())\n",
        "    mask[np.tril_indices_from(mask)] = True\n",
        "    \n",
        "    with sns.axes_style(\"white\"):\n",
        "        sns.heatmap(data.corr(), mask=mask, annot=True, cmap=\"coolwarm\")\n",
        "    \n",
        "    if title: plt.title(f\"\\n{title}\\n\", fontsize=18)\n",
        "    plt.show()\n",
        "    return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "T6hEZSGZY9uu"
      },
      "source": [
        "half_corr_heatmap(data_2, 'Correlation Between Variables')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWr_-hx0ZFk0"
      },
      "source": [
        "There is no high correlation between variables.\n",
        "So let's try to identify how each feature is correlated with the outcome (0,1)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "4eSXUSN8ZUW2"
      },
      "source": [
        "def corr_to_target(dataframe, target, title=None, file=None):\n",
        "    plt.figure(figsize=(4,6))\n",
        "    sns.set(font_scale=1)\n",
        "    \n",
        "    sns.heatmap(dataframe.corr()[[target]].sort_values(target,\n",
        "                                                ascending=False)[1:],\n",
        "                annot=True,\n",
        "                cmap='coolwarm')\n",
        "    \n",
        "    if title: plt.title(f'\\n{title}\\n', fontsize=18)\n",
        "    plt.xlabel('')    # optional in case you want an x-axis label\n",
        "    plt.ylabel('')    # optional in case you want a  y-axis label\n",
        "    if file: plt.savefig(file, bbox_inches='tight')\n",
        "    plt.show();\n",
        "    \n",
        "    return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "A_gaYzV9ZXIZ"
      },
      "source": [
        "corr_to_target(data_2, 'Outcome', 'Outcome');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2prRTquzZdD-"
      },
      "source": [
        "As we can see below, Glucose is highly correlated with outcome, followed by BMI and Age."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pY8PdTKwaOkZ"
      },
      "source": [
        "**Handling Outliers**\n",
        "\n",
        "Let's create boxplots for all variables in dataframe, in order to detect outliers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "6Nke11XSaWwv"
      },
      "source": [
        "def gen_boxplots(dataframe, cols=1, file=None):\n",
        "    rows      = math.ceil(len(dataframe.columns)/cols)\n",
        "    figwidth  = 5 * cols\n",
        "    figheight = 4 * rows\n",
        "\n",
        "    fig, ax = plt.subplots(nrows   = rows,\n",
        "                           ncols   = cols,\n",
        "                           figsize = (figwidth, figheight))\n",
        "    \n",
        "    plt.subplots_adjust(wspace=0.3, hspace=0.3)\n",
        "    ax = ax.ravel()         # Ravel turns a matrix into a vector... easier to iterate\n",
        "\n",
        "    for i, column in enumerate(dataframe.columns):\n",
        "        ax[i].boxplot(dataframe[column])\n",
        "        ax[i].set_title(f'{dataframe[column].name}', fontsize=18)\n",
        "        ax[i].set_ylabel('', fontsize=14)\n",
        "        ax[i].set_xlabel('', fontsize=14)\n",
        "        ax[i].tick_params(labelbottom=False)\n",
        "        \n",
        "    fig.suptitle('\\nBoxplots for All Variables in Dataframe', size=24)\n",
        "    fig.tight_layout()\n",
        "    fig.subplots_adjust(bottom=0, top=0.88)\n",
        "    if file: plt.savefig(file, bbox_inches='tight')\n",
        "    plt.show();\n",
        "\n",
        "    return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "RmI6pF_uajRq"
      },
      "source": [
        "import math\n",
        "gen_boxplots(data_2, 3);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2dQ9iARbFOJ"
      },
      "source": [
        "Insulin, DiabetesPedigreeFunction, BMI, SkinThickness all have outliers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "VnsFzKeabBjL"
      },
      "source": [
        "data_2.groupby('Outcome')[['BMI', 'Age', 'Insulin', 'Pregnancies']].agg(['min', 'max', 'mean'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5gZlMFAbRe9"
      },
      "source": [
        "BMI should be between 30, and 34.9, to be classified as obese. Greater than 35 shows that we have extreme obeseity. As we see above, mean of the BMI values shows us that excess weight can cause diabetes.\n",
        "With age mean, which 37 is very young, means that high weight, and young age with diabetes is in the majority in this data.\n",
        "Someone has 13, and 17 children which is incredible. Since, we do not know how many of them has children with that amount, we cannot decide whether this is decisive for them to being diabetes. But, average number of children across two possibilities is low."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCHbUnmkcH0T"
      },
      "source": [
        "Now let's focus on DiabetesPedigreeFunction: It provides information about diabetes history in relatives and genetic relationship of those relatives with patients. Higher Pedigree Function means patient is more likely to have diabetes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "TsYHkmaxno5V"
      },
      "source": [
        "sns.scatterplot(data = data_2, x = 'DiabetesPedigreeFunction', y = 'Pregnancies', hue = 'Outcome');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dvg8dR4xoEkE"
      },
      "source": [
        "**Numerical Column Grouping**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "qM0ma8R9n0ul"
      },
      "source": [
        "data_2.groupby('Pregnancies').Pregnancies.count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "oiASbhz-oQL6"
      },
      "source": [
        "data_2.groupby('Pregnancies').size().plot(kind = 'line', color = 'red', linewidth = 1.2);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "GfMW9dZVoQga"
      },
      "source": [
        "sns.countplot(data_2['Outcome']);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_iFnKA4-ogyX"
      },
      "source": [
        "**Removing outliers**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Th2qhjAXopMQ"
      },
      "source": [
        "Q1 = data_2.quantile(0.25)\n",
        "Q2 = data_2.quantile(0.75)\n",
        "IQR = Q2 - Q1\n",
        "IQR"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "dmk7Qs2Xothn"
      },
      "source": [
        "data2_out = data_2[~((data_2 < (Q1 - 1.5 * IQR)) |(data_2 > (Q2 + 1.5 * IQR))).any(axis=1)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "0TGzVYcNowsA"
      },
      "source": [
        "data2_out.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "M8iKZWcNozel"
      },
      "source": [
        "gen_boxplots(data2_out,3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M2a_IG7ypBto"
      },
      "source": [
        "Now let's use scatterplots and histograms for the two outcomes:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "JJtd5BWvpAx_"
      },
      "source": [
        "g = sns.FacetGrid(data2_out, col=\"Outcome\", height=3.5, aspect=1.6)\n",
        "g.map_dataframe(sns.scatterplot, x=\"Glucose\", y=\"Insulin\", hue=\"Pregnancies\", size = 'Pregnancies', sizes=(20, 200))\n",
        "g.set_axis_labels(\"Glucose Level\", \"Insulin Level\")\n",
        "g.add_legend();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "IqQB7zuTpWrs"
      },
      "source": [
        "g2 = sns.FacetGrid(data2_out, col=\"Outcome\", height=3.5, aspect=1.6)\n",
        "g2.map(sns.countplot, \"Pregnancies\", color = 'red');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "MmN_wifEpdOw"
      },
      "source": [
        "g3 = sns.FacetGrid(data2_out, col=\"Outcome\", height=3.5, aspect=1.6)\n",
        "g3.map(sns.distplot, \"BMI\", color = 'red');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "2u8S2L4YpjP3"
      },
      "source": [
        "g4 = sns.FacetGrid(data2_out, col=\"Outcome\", height=3.5, aspect=1.6)\n",
        "g4.map(sns.distplot, \"SkinThickness\", color = 'red');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90JgsVaopqIF"
      },
      "source": [
        "Let's gather all together to get the big picture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "cqa86BFkpxqq"
      },
      "source": [
        "import seaborn as sns\n",
        "sns.pairplot(data = data2_out, hue = 'Outcome');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Bs5-3MfohVf"
      },
      "source": [
        "**MODEL BUILDING**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "5-d6Q6zVMiBe"
      },
      "source": [
        "x=dataset.iloc[:,: -1]\n",
        "y=dataset.iloc[:,-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WshbEBUtNcDw"
      },
      "source": [
        "**Data splitting**\n",
        "\n",
        "Here the data set has been divided into train and test data set. The test data set size is 33% of the total records. This test data will not be used in model training and work as an independent test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "W_q2aD_lNm9B"
      },
      "source": [
        "X = data2_out.drop('Outcome', axis = 1)\n",
        "y = data2_out['Outcome']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "flZfP7eL7BlM"
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "53kdTnar7Dti"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .33, random_state = 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i15Jfhg968C9"
      },
      "source": [
        "**StandardScaler**\n",
        "\n",
        "StandardScaler is an important technique that is mainly performed as a preprocessing step before many machine learning models, in order to standardize the range of functionality of the input dataset. StandardScaler comes into play when the characteristics of the input dataset differ greatly between their ranges, or simply when they are measured in different units of measure."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "xzsv1ELT7E2g"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "hNSOCJ4W7HQe"
      },
      "source": [
        "sc = StandardScaler()\n",
        "X_train_sc = sc.fit_transform(X_train)\n",
        "X_test_sc = sc.transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZS8Ik-zJxFw"
      },
      "source": [
        "***LOGISTIC REGRESSION***\n",
        "\n",
        "Logistic regression is useful for situations in which you want to be able to predict the presence or absence of a characteristic or outcome based on values of a set of predictor variables. It is similar to a linear regression model but is suited to models where the dependent variable is dichotomous. Logistic regression coefficients can be used to estimate odds ratios for each of the independent variables in the model. Logistic regression is applicable to a broader range of research situations than discriminant analysis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "xTukxDDyKsn0"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "aAsakeMCSuPF"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "lr = LogisticRegression(max_iter = 1000)\n",
        "lr.fit(X_train_sc,y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ftzhhyn7S-6I"
      },
      "source": [
        "pred = lr.predict(X_test_sc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ssxsNF2-UoyE"
      },
      "source": [
        "**Evaluation Metrics for Classification:**\n",
        "**Accuracy Value**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "_qOKc-vb_pOC"
      },
      "source": [
        "from sklearn import metrics\n",
        "from sklearn.metrics import accuracy_score , precision_score, recall_score, f1_score, roc_auc_score, roc_curve, precision_recall_curve"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "XPchKe6yTExr"
      },
      "source": [
        "print(lr.score(X_test_sc,y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g494W1A-U5-_"
      },
      "source": [
        "See that the model has 77% accuracy which is pretty good.\n",
        "If you want to improve this accuracy, logistic regression contains many hyperparameters that you can tune to improve it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qHHDuQoXs3eo"
      },
      "source": [
        "**Hypermapameter optimization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "nJRFm6h3Tgz5"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "confusion_matrix = confusion_matrix(y_test, pred)\n",
        "print(confusion_matrix)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxsIZt1tV9cw"
      },
      "source": [
        "*Precision and Recall*\n",
        "\n",
        "Recall: Gives information about how much we have predicted correctly for a real positive class.\n",
        "Precision: Gives information about how much positive class we have predicted as positive are actually positive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "y0fUx_sWTlly"
      },
      "source": [
        "from sklearn.metrics import precision_score, recall_score\n",
        "print(\"Precision score:\",precision_score(y_test, pred))\n",
        "print(\"Recall score:\",recall_score(y_test, pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oeKhVXgEWEBA"
      },
      "source": [
        "*Roc_Auc_Score*\n",
        "\n",
        "A receiver operating characteristic curve, or ROC curve, is a graphical plot that illustrates the diagnostic ability of a binary classifier across different threshold values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "vdvUGuuQUQlB"
      },
      "source": [
        "from sklearn.metrics import roc_curve,roc_auc_score\n",
        "y_pred_proba = lr.predict_proba(X_test_sc)[::,1]\n",
        "fpr, tpr, _ = roc_curve(y_test,  pred)\n",
        "auc = roc_auc_score(y_test, y_pred_proba)\n",
        "plt.plot(fpr,tpr,label=\"auc=\"+str(auc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "5JQcCfoGBMOD"
      },
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "auc = roc_auc_score(y_test, y_pred_proba)\n",
        "print(\"Roc auc score:\",roc_auc_score(y_test, y_pred_proba))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sGB_t0VzWUSV"
      },
      "source": [
        "*GridSearchCV*\n",
        "\n",
        "GridSearchCV implements the most obvious way of finding an optimal value for anything — it simply tries all the possible values (that you pass) one at a time and returns which one yielded the best model results, based on the scoring that you want, such as accuracy on the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Q0ECxiAxKwdL"
      },
      "source": [
        "gscv = GridSearchCV(LogisticRegression(solver='liblinear', multi_class = 'auto'), \n",
        "                    {'C' : [1, 10, 20]}, \n",
        "                    cv = 5, return_train_score=False)\n",
        "gscv.fit(X, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "SYZXFjYTKz2B"
      },
      "source": [
        "gscv.best_params_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yaQuZkuyLWdj"
      },
      "source": [
        "*Model Tuning*\n",
        "\n",
        "Fine tuning machine learning predictive model is a crucial step to improve accuracy of the forecasted results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "QwGCJK06LtQB"
      },
      "source": [
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, roc_curve, roc_auc_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "9I_HPpIeLUlf"
      },
      "source": [
        "log = LogisticRegression(C = 10)\n",
        "log.fit(X_train_sc, y_train)\n",
        "log_y_pred = log.predict(X_test_sc)\n",
        "log_y_pred_train = log.predict(X_train_sc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "25OUKmmQLe4a"
      },
      "source": [
        "log_as = accuracy_score(y_test, log_y_pred)\n",
        "log_as_train = accuracy_score(log_y_pred_train, y_train)\n",
        "log_as_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "5t6jM3w6L5bK"
      },
      "source": [
        "log_as"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "6bD_MxdhMIKf"
      },
      "source": [
        "print(classification_report(log_y_pred, y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "W25G7MadV3vP"
      },
      "source": [
        "print(f\"Logistic Regression model accuracy score for test data : {log_as}\")\n",
        "print(f\"Logistic Regression model accuracy score for train data : {log_as_train}\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZKAw4ZtLw55"
      },
      "source": [
        "***ADDING POLYNOMIAL EFFECT***\n",
        "\n",
        "Polynomial regression also a type of linear regression is often used to make predictions using polynomial powers of the independent variables.\n",
        "In the case of simple linear regression, there is some data that is above or below the line and thus it’s not accurate. This is where polynomial regression can be used."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "byv5QtYOMQIO"
      },
      "source": [
        "*Preprocessing Data*\n",
        "\n",
        "This is the additional step we apply to polynomial regression, where we add the feature 𝑥² to our Model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gAe9cQzqPbfA"
      },
      "source": [
        "First we need to convert your data to polynomial features. Originally, our data has 8 columns."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "V6wTajHnMU7z"
      },
      "source": [
        "X_train_sc.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YE0fBXffPfMu"
      },
      "source": [
        "We create the polynomial features with scikit learn (here it is for degree 2):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "vJ241lNVPStB"
      },
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "poly = PolynomialFeatures(degree = 2, interaction_only=False, include_bias=False)\n",
        "X_poly = poly.fit_transform(X_train_sc)\n",
        "X_poly.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuxrH58WPozV"
      },
      "source": [
        "We know have 44 features. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BxXF1F-oPvho"
      },
      "source": [
        "On this you can now build your logistic regression calling X_poly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "EYBF7jFrPxK7"
      },
      "source": [
        "lr = LogisticRegression()\n",
        "lr.fit(X_poly,y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tVmqEwXZQcUj"
      },
      "source": [
        "We evaluate your model on the test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "rzfd1VpiP83z"
      },
      "source": [
        "lr.score(poly.transform(X_test_sc), y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MO-AVQAGuEVL"
      },
      "source": [
        "As we can see, polynomial regression diminished the accuracy score at 71%."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FOmlzWuNcyJ"
      },
      "source": [
        "**L1 and L2 REGULARIZATION**\n",
        "\n",
        "Ridge and Lasso regression are some of the simple techniques to reduce model complexity and prevent over-fitting which may result from simple linear regression."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCrI5msKNs-I"
      },
      "source": [
        "*Ridge Regression*\n",
        "\n",
        "In ridge regression, the cost function is altered by adding a penalty equivalent to square of the magnitude of the coefficients."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "0oBWAOmwZIKC"
      },
      "source": [
        "from sklearn.linear_model import Ridge\n",
        "rr = Ridge(alpha=0.01) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ie1KzGcgZL5E"
      },
      "source": [
        "rr.fit(X_train_sc, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "JwM448BHZcJI"
      },
      "source": [
        "rr100 = Ridge(alpha=100) #  comparison with alpha value\n",
        "rr100.fit(X_train_sc, y_train)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Nqt8sjJDZhie"
      },
      "source": [
        "Ridge_train_score = rr.score(X_train_sc,y_train)\n",
        "Ridge_test_score = rr.score(X_test_sc, y_test)\n",
        "Ridge_train_score100 = rr100.score(X_train_sc,y_train)\n",
        "Ridge_test_score100 = rr100.score(X_test_sc, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "YT-2lVDNZstT"
      },
      "source": [
        "print(f\"Ridge score for train data : {Ridge_train_score}\")\n",
        "print(f\"Ridge score for test data : {Ridge_test_score}\")\n",
        "print(f\"Ridge score for train data 100 : {Ridge_train_score100}\")\n",
        "print(f\"Ridge score for test data 100: {Ridge_test_score100}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "NaKWm9PeadWE"
      },
      "source": [
        "plt.plot(rr.coef_,alpha=0.7,linestyle='none',marker='*',markersize=5,color='red',label=r'Ridge; $\\alpha = 0.01$',zorder=7) \n",
        "plt.plot(rr100.coef_,alpha=0.5,linestyle='none',marker='d',markersize=6,color='blue',label=r'Ridge; $\\alpha = 100$') \n",
        "plt.xlabel('Coefficient Index',fontsize=16)\n",
        "plt.ylabel('Coefficient Magnitude',fontsize=16)\n",
        "plt.legend(fontsize=9, loc=0)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqej5bdfjeF8"
      },
      "source": [
        "**Lasso regression** \n",
        "\n",
        "It not only helps in reducing over-fitting but it can help us in feature selection. Just like Ridge regression the regularization parameter (lambda) can be controlled and we will see the effect below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "QremgYPojwE0"
      },
      "source": [
        "from sklearn.linear_model import Lasso"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "lcZc8WWOjnZQ"
      },
      "source": [
        "lasso = Lasso()\n",
        "lasso.fit(X_train,y_train)\n",
        "train_score=lasso.score(X_train,y_train)\n",
        "test_score=lasso.score(X_test,y_test)\n",
        "coeff_used = np.sum(lasso.coef_!=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "NRTWwfaEj8Mf"
      },
      "source": [
        "print (\"training score:\", train_score)\n",
        "print (\"test score:\", test_score)\n",
        "print (\"number of features used: \", coeff_used)\n",
        "lasso001 = Lasso(alpha=0.01, max_iter=10e5)\n",
        "lasso001.fit(X_train,y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "x-d5mNkAvcN2"
      },
      "source": [
        "train_score001=lasso001.score(X_train,y_train)\n",
        "test_score001=lasso001.score(X_test,y_test)\n",
        "coeff_used001 = np.sum(lasso001.coef_!=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "v9aAYr3yviWL"
      },
      "source": [
        "print (\"training score for alpha=0.01:\", train_score001) \n",
        "print (\"test score for alpha=0.01: \", test_score001)\n",
        "print (\"number of features used: for alpha=0.01:\", coeff_used001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Umfig6SqvyEN"
      },
      "source": [
        "lasso00001 = Lasso(alpha=0.0001, max_iter=10e5)\n",
        "lasso00001.fit(X_train,y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "MIu2mhLzv2i4"
      },
      "source": [
        "train_score00001=lasso00001.score(X_train,y_train)\n",
        "test_score00001=lasso00001.score(X_test,y_test)\n",
        "coeff_used00001 = np.sum(lasso00001.coef_!=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "I3PLM8amv7Jd"
      },
      "source": [
        "print (\"training score for alpha=0.0001:\", train_score00001) \n",
        "print (\"test score for alpha =0.0001: \", test_score00001)\n",
        "print (\"number of features used: for alpha =0.0001:\", coeff_used00001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ptk6tIaIwHf7"
      },
      "source": [
        "plt.subplot(1,2,1)\n",
        "plt.plot(lasso.coef_,alpha=0.7,linestyle='none',marker='*',markersize=5,color='red',label=r'Lasso; $\\alpha = 1$',zorder=7) # alpha here is for transparency\n",
        "plt.plot(lasso001.coef_,alpha=0.5,linestyle='none',marker='d',markersize=6,color='blue',label=r'Lasso; $\\alpha = 0.01$') # alpha here is for transparency\n",
        "\n",
        "plt.xlabel('Coefficient Index',fontsize=16)\n",
        "plt.ylabel('Coefficient Magnitude',fontsize=16)\n",
        "plt.legend(fontsize=13,loc=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "LcSYDHiewRuO"
      },
      "source": [
        "plt.subplot(1,2,2)\n",
        "plt.plot(lasso.coef_,alpha=0.7,linestyle='none',marker='*',markersize=5,color='red',label=r'Lasso; $\\alpha = 1$',zorder=7) # alpha here is for transparency\n",
        "plt.plot(lasso001.coef_,alpha=0.5,linestyle='none',marker='d',markersize=6,color='blue',label=r'Lasso; $\\alpha = 0.01$') # alpha here is for transparency\n",
        "plt.plot(lasso00001.coef_,alpha=0.8,linestyle='none',marker='v',markersize=6,color='black',label=r'Lasso; $\\alpha = 0.00001$') # alpha here is for transparency\n",
        "plt.xlabel('Coefficient Index',fontsize=16)\n",
        "plt.ylabel('Coefficient Magnitude',fontsize=16)\n",
        "plt.legend(fontsize=13,loc=0)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLiRPzXfumLv"
      },
      "source": [
        "Let’s understand the plot and the code in a short summary.\n",
        "\n",
        "The default value of regularization parameter in Lasso regression (given by α) is 1.\n",
        "\n",
        "With this, out of 8 features in cancer data-set, only 1 feature is used (non zero value of the coefficient).\n",
        "\n",
        "Both training and test score (with only 1 feature) are low; conclude that the model is under-fitting the diabetes data-set.\n",
        "\n",
        "Reduce this under-fitting by reducing alpha and increasing number of iterations. Now α = 0.01, non-zero features =8, training and test score increases.\n",
        "\n",
        "Comparison of coefficient magnitude for two different values of alpha are shown in the left panel of figure 2. For alpha =1, we can see most of the coefficients are zero or nearly zero, which is not the case for alpha=0.01.\n",
        "\n",
        "Further reduce α =0.0001, non-zero features = 8. Training and test scores are always fairly low compared to basic logistic regression.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQAU2ecf-S7u"
      },
      "source": [
        "**CONCLUSION**\n",
        "\n",
        "We explored the dataset and we arrived to the conclusion that the features prensent low correlation between each other. Just Glucose levels, BMI and Age present a medium-low level of correlation with result (diabetes condition vs not).\n",
        "After a long process of Handling outliers and dropping null values we splitted the dataset and we have choosen the Logistic regression model to analyze the entire dataset to make prediction. We got a pretty good accuracy rate of 77% using Hyperparameter optimization and model tuning.\n",
        "Altough, using the polynomial effect we reached an accuracy rate of 71%, so it decreased. Moreover, using Lasso and Ridge regularization, the accuracy dropped at low rates between 22% and 32% for the training set, and between 24% and 28% for test set. This effect is probably due to underfitting. Underfitting occurs when our model fails to capture the underlying trend in our data. Models which underfit our data like in this case have a low Variance and a high Bias and tend to have less features. \n",
        "Possible ways to increase the accuracy rate avoiding underfitting are changing algorithm models like as SVM that work better with dataset with limited number of features.  "
      ]
    }
  ]
}